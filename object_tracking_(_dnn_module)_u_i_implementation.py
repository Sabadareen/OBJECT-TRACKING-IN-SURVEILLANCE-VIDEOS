# -*- coding: utf-8 -*-
"""Object Tracking ( DNN module) U.I. Implementation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VB56u36Y5UzDQSC8wZdckKT_XVVG_0q3
"""

pip install gradio opencv-python-headless

import gradio as gr
import cv2
import numpy as np

def track_objects_no_detection(video_path):
    # Open the video file
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        return "Error: Unable to load the video file."

    # Get video properties
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    # Output video settings
    output_path = "processed_video.mp4"
    output_video = cv2.VideoWriter(
        output_path, cv2.VideoWriter_fourcc(*"mp4v"), fps, (frame_width, frame_height)
    )

    # Initialize variables for tracking
    ret, prev_frame = cap.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    tracking_objects = {}
    track_id = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # Compute the difference between current and previous frame
        diff = cv2.absdiff(prev_gray, gray)
        _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)

        # Find contours of moving objects
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        center_points_cur_frame = []

        for contour in contours:
            if cv2.contourArea(contour) < 500:  # Skip small areas
                continue

            x, y, w, h = cv2.boundingRect(contour)
            cx = int(x + w / 2)
            cy = int(y + h / 2)
            center_points_cur_frame.append((cx, cy))
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

        # Update tracking objects
        for pt in center_points_cur_frame:
            same_object_detected = False
            for object_id, prev_pt in tracking_objects.items():
                distance = np.linalg.norm(np.array(prev_pt) - np.array(pt))
                if distance < 35:
                    tracking_objects[object_id] = pt
                    same_object_detected = True
                    break

            if not same_object_detected:
                tracking_objects[track_id] = pt
                track_id += 1

        # Draw tracking points and IDs
        for object_id, pt in tracking_objects.items():
            cv2.circle(frame, pt, 5, (0, 0, 255), -1)
            cv2.putText(frame, str(object_id), (pt[0] - 10, pt[1] - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        output_video.write(frame)
        prev_gray = gray

    cap.release()
    output_video.release()
    return output_path

# Define Gradio Interface
def gradio_interface(video):
    processed_video_path = track_objects_no_detection(video)
    return processed_video_path

# Gradio app
iface = gr.Interface(
    fn=gradio_interface,
    inputs=gr.Video(label="Upload a video"),
    outputs=gr.Video(label="Processed Video with Object Tracking"),
    title="Object Tracking without Detection",
    description="Upload a video, and the application will track moving objects based on motion detection."
)

iface.launch()